# @package _global_

defaults:
    # - /alg: supmatch_no_disc
    - override /dm: acs
    - override /ds: acs/employment_dis_fl
    - override /split: acs/employment_dis
    - override /labeller: gt
    - override /ae_arch: fcn

alg:
    use_amp: False
    pred:
        lr: ${ ae.lr }
        optimizer_cls: ${ ae.optimizer_cls }
        # weight_decay: ${ ae.weight_decay }
    steps: 10000
    val_freq: 1000
    log_freq: ${ alg.steps }
    num_disc_updates: 3
    disc_loss_w: 0.03
    # ga_steps: 1
    # max_grad_norm: null

ae:
    recon_loss: l2
    zs_dim: 1

ae_opt:
    lr: 1.e-4
    optimizer_cls: ADAM
    weight_decay: 1.e-2

ae_arch:
    hidden_dim: 95
    latent_dim: 64
    num_hidden: 3
    dropout_prob: 0.1

eval:
    batch_size: 128
    model:
        num_hidden: 0
    opt:
        lr: 5.e-4
        optimizer_cls: ADAM
        # scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
        # scheduler_kwargs:
        #     T_max: ${ eval.steps }
        #     eta_min: 5e-7
