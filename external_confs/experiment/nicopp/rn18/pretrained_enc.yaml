# @package _global_

defaults:
    - override /ds: nicopp
    - override /split: nicopp/change_is_hard
    - override /dm: nicopp
    - override /labeller: gt
    - override /ae_arch: resnet
    - _self_

ae_arch:
    version: RN18
    first_conv: true
    maxpool1: true
    latent_dim: 256
    pretrained_enc: true

alg:
    use_amp: true
    lr: 1.e-5
    log_freq: 100000000000  # never
    val_freq: 200
    num_disc_updates: 5
    enc_loss_w: 0.0001
    disc_loss_w: 0.03
    prior_loss_w: 0.01
    pred_y_loss_w: 1
    pred_s_loss_w: 0
    s_pred_with_bias: false
    s_as_zs: false

ae:
    zs_dim: 6
    zs_transform: none

disc:
    lr: 1.e-4

eval:
    batch_size: 1
    balanced_sampling: true
    hidden_dim: null
    num_hidden: 1
    steps: 10000
    opt:
        lr: 1.e-4
        scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
        scheduler_kwargs:
            T_max: ${ eval.steps }
            eta_min: 5e-7
